<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
                "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
  <title>Description of ml_calcloss</title>
  <meta name="keywords" content="ml_calcloss">
  <meta name="description" content="Calculate the loss for a set of predictions, given knowledge about the target values.">
  <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
  <meta name="generator" content="m2html v1.5 &copy; 2003-2005 Guillaume Flandin">
  <meta name="robots" content="index, follow">
  <link type="text/css" rel="stylesheet" href="../../m2html.css">
</head>
<body>
<a name="_top"></a>
<div><a href="../../index.html">Home</a> &gt;  <a href="#">code</a> &gt; <a href="index.html">machine_learning</a> &gt; ml_calcloss.m</div>

<!--<table width="100%"><tr><td align="left"><a href="../../index.html"><img alt="<" border="0" src="../../left.png">&nbsp;Master index</a></td>
<td align="right"><a href="index.html">Index for code/machine_learning&nbsp;<img alt=">" border="0" src="../../right.png"></a></td></tr></table>-->

<h1>ml_calcloss

</h1>

<h2><a name="_name"></a>PURPOSE <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
<div class="box"><strong>Calculate the loss for a set of predictions, given knowledge about the target values.</strong></div>

<h2><a name="_synopsis"></a>SYNOPSIS <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
<div class="box"><strong>function [measure,stats] = ml_calcloss(type,T,P) </strong></div>

<h2><a name="_description"></a>DESCRIPTION <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
<div class="fragment"><pre class="comment"> Calculate the loss for a set of predictions, given knowledge about the target values.
 [Loss,Stats] = ml_calcloss(Type, Target, Prediction)

 The performance of predictive models on data [1], or the support of hypotheses given data can be estimated with help of this function, which compares
 a set of target values with predicted values (which are supposed to match the targets) and computes the 'loss' (or cost, or degree of failure). 
 Since both targets and predictions can assume categorical, continuous or multivariate values, or probability distributions over such values, 
 there is a variety of different loss functions to chose from. Depending on the data types of the targets and predictions, a reasonable choice
 of default loss function can often be auto-selected by the function (if Type is specified as []), for example mis-classification rate for categorical
 variables, mean square error for continuous variables, etc.. 

 Ultimately, the loss function should reflect most closely the actual loss incurred by a wrong prediction, in the context where the method is supposed to
 be used. For example, if a predictive model that shall predict either +1 or -1 can assign (gradual) probabilities to the two possible outcomes, the most 
 informative loss measure would be the misfit between the actual and predicted probabiltities of those outcomes. If an application maps the output to
 a one-dimensional control signal (e.g. turning a robot gradually left or right), this loss measure would be appropriate (or mean-square error could also 
 be used). If the application, however, makes an exact binary decision based on the probabilities (e.g. playing a sound in the +1 case and not playing
 it otherwise), then the most true-to-the-fact loss function would be the mis-classification rate, and consequently, a model optimized under this 
 loss can be assumed to work better in practice, even though the loss does not capture all information present in the target &amp; prediction values. If the
 relative cost of wrongfully chosing one class (e.g., play sound) is different from that of wrongfully chosing the other class (play no sound), and is 
 not fixed/known in advance, an even better loss would be the (negative) area under the ROC (Receiver-Operating Characteristic) curve.
 
 In:
   Type       : the type of loss to compute:
                * 'auto' / empty: use 'mcr','mse','nll','kld', depending on supplied target &amp; prediction data formats
                   * misclassification rate is used for discrete probability distributions,
                   * mean squared error is used for regression outputs
                   * negative log-likelihood is used for continuous probability distributions
                   * kullback-leibler divergence is used when both targets and predictions are distributions
                * 'kld': Kullback-Leibler divergence D_KL(Target||Prediction)
                * 'nll': negative log-likelihood of target under prediction or vice versa (depending on what is specified as a distribution)
                * 'mcr'/'err': misclassification rate, for discrete outcomes
                * 'mae'/'l1': mean absolute error
                * 'mse'/'l2': mean square error
                * 'smse': standardized mean square error
                * 'smae': standardized mean absolute error
                * 'max'/'linf': maximum absolute error
                * 'sign': mis-classification rate on the sign of continuous outcomes
                * 'rms': root mean square error
                * 'bias': directed mean bias
                * 'medse': median square error
                * 'medae': median absolute error
                * 'smedae': standardized median absolute error (also using a robust std. dev,)
                * 'auc': negative area under ROC, for 2-class outcomes
                * 'cond_entropy': conditional entropy of Target given Prediction, for discrete outcomes
                * 'cross_entropy': cross-entropy of Target under Prediction
                * 'f_measure': negative F-score, harmonic mean between precision and recall, for 2-class outcomes
                * cell array of strings: compute loss for each specified type and return an array of results

   Target     : target variable; can be in any of the formats produced by ml_predict.

   Prediction : predicted variable; can be in any of the formats produced by ml_predict.
                format is expected to be consistent with (though not necessarily identical to) the Target format, 
                and the number of samples needs to match that of the target variable
                
 Out:
   Loss       : the scalar loss, aggregated over all samples, or a vector of loss measures, if a cell array was specified for the type
   Stats      : struct of additional statistics (depending on type), or a struct array if a cell array was specified for the type

 Note:
   Simple of the implemented losses have not been tested for all applicable combinations of target/prediction formats. For standard applications,
   there should be no errors, however. When new losses are implemented, they should be added to this file.

 References:
   [1] MacKay, D. J. C. &quot;Information theory, inference, and learning algorithms.&quot; 
       Cambridge University Press, 2003.

                               Christian Kothe, Swartz Center for Computational Neuroscience, UCSD
                               2010-04-22</pre></div>

<!-- crossreference -->
<h2><a name="_cross"></a>CROSS-REFERENCE INFORMATION <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
This function calls:
<ul style="list-style-image:url(../../matlabicon.gif)">

<li><a href="ml_calcloss.html" class="code" title="function [measure,stats] = ml_calcloss(type,T,P)">ml_calcloss</a>	Calculate the loss for a set of predictions, given knowledge about the target values.</li>
</ul>
This function is called by:
<ul style="list-style-image:url(../../matlabicon.gif)">

<li><a href="ml_calcloss.html" class="code" title="function [measure,stats] = ml_calcloss(type,T,P)">ml_calcloss</a>	Calculate the loss for a set of predictions, given knowledge about the target values.</li>
<li><a href="ml_traindal.html" class="code" title="function model = ml_traindal(varargin)">ml_traindal</a>	Learn a linear probabilistic model via the Dual-Augmented Lagrangian method.</li>
<li><a href="ml_trainproximal.html" class="code" title="function model = ml_trainproximal(varargin)">ml_trainproximal</a>	Learn a linear probabilistic model proximal splitting methods.</li>
</ul>
<!-- crossreference -->


<h2><a name="_subfunctions"></a>SUBFUNCTIONS <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
<ul style="list-style-image:url(../../matlabicon.gif)">

<li><a href="#_sub1" class="code">function Y = mad(X,flag)</a></li>
<li><a href="#_sub2" class="code">function V = expected_value(D)</a></li>
<li><a href="#_sub3" class="code">function V = neg_loglike(X,D,maxval)</a></li>
<li><a href="#_sub4" class="code">function res = kl_divergence_discrete(P,Q)</a></li>
<li><a href="#_sub5" class="code">function res = kl_divergence_gaussian(P,Q)</a></li>
</ul>




<h2><a name="_source"></a>SOURCE CODE <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
<div class="fragment"><pre>0001 <a name="_sub0" href="#_subfunctions" class="code">function [measure,stats] = ml_calcloss(type,T,P)</a>
0002 <span class="comment">% Calculate the loss for a set of predictions, given knowledge about the target values.</span>
0003 <span class="comment">% [Loss,Stats] = ml_calcloss(Type, Target, Prediction)</span>
0004 <span class="comment">%</span>
0005 <span class="comment">% The performance of predictive models on data [1], or the support of hypotheses given data can be estimated with help of this function, which compares</span>
0006 <span class="comment">% a set of target values with predicted values (which are supposed to match the targets) and computes the 'loss' (or cost, or degree of failure).</span>
0007 <span class="comment">% Since both targets and predictions can assume categorical, continuous or multivariate values, or probability distributions over such values,</span>
0008 <span class="comment">% there is a variety of different loss functions to chose from. Depending on the data types of the targets and predictions, a reasonable choice</span>
0009 <span class="comment">% of default loss function can often be auto-selected by the function (if Type is specified as []), for example mis-classification rate for categorical</span>
0010 <span class="comment">% variables, mean square error for continuous variables, etc..</span>
0011 <span class="comment">%</span>
0012 <span class="comment">% Ultimately, the loss function should reflect most closely the actual loss incurred by a wrong prediction, in the context where the method is supposed to</span>
0013 <span class="comment">% be used. For example, if a predictive model that shall predict either +1 or -1 can assign (gradual) probabilities to the two possible outcomes, the most</span>
0014 <span class="comment">% informative loss measure would be the misfit between the actual and predicted probabiltities of those outcomes. If an application maps the output to</span>
0015 <span class="comment">% a one-dimensional control signal (e.g. turning a robot gradually left or right), this loss measure would be appropriate (or mean-square error could also</span>
0016 <span class="comment">% be used). If the application, however, makes an exact binary decision based on the probabilities (e.g. playing a sound in the +1 case and not playing</span>
0017 <span class="comment">% it otherwise), then the most true-to-the-fact loss function would be the mis-classification rate, and consequently, a model optimized under this</span>
0018 <span class="comment">% loss can be assumed to work better in practice, even though the loss does not capture all information present in the target &amp; prediction values. If the</span>
0019 <span class="comment">% relative cost of wrongfully chosing one class (e.g., play sound) is different from that of wrongfully chosing the other class (play no sound), and is</span>
0020 <span class="comment">% not fixed/known in advance, an even better loss would be the (negative) area under the ROC (Receiver-Operating Characteristic) curve.</span>
0021 <span class="comment">%</span>
0022 <span class="comment">% In:</span>
0023 <span class="comment">%   Type       : the type of loss to compute:</span>
0024 <span class="comment">%                * 'auto' / empty: use 'mcr','mse','nll','kld', depending on supplied target &amp; prediction data formats</span>
0025 <span class="comment">%                   * misclassification rate is used for discrete probability distributions,</span>
0026 <span class="comment">%                   * mean squared error is used for regression outputs</span>
0027 <span class="comment">%                   * negative log-likelihood is used for continuous probability distributions</span>
0028 <span class="comment">%                   * kullback-leibler divergence is used when both targets and predictions are distributions</span>
0029 <span class="comment">%                * 'kld': Kullback-Leibler divergence D_KL(Target||Prediction)</span>
0030 <span class="comment">%                * 'nll': negative log-likelihood of target under prediction or vice versa (depending on what is specified as a distribution)</span>
0031 <span class="comment">%                * 'mcr'/'err': misclassification rate, for discrete outcomes</span>
0032 <span class="comment">%                * 'mae'/'l1': mean absolute error</span>
0033 <span class="comment">%                * 'mse'/'l2': mean square error</span>
0034 <span class="comment">%                * 'smse': standardized mean square error</span>
0035 <span class="comment">%                * 'smae': standardized mean absolute error</span>
0036 <span class="comment">%                * 'max'/'linf': maximum absolute error</span>
0037 <span class="comment">%                * 'sign': mis-classification rate on the sign of continuous outcomes</span>
0038 <span class="comment">%                * 'rms': root mean square error</span>
0039 <span class="comment">%                * 'bias': directed mean bias</span>
0040 <span class="comment">%                * 'medse': median square error</span>
0041 <span class="comment">%                * 'medae': median absolute error</span>
0042 <span class="comment">%                * 'smedae': standardized median absolute error (also using a robust std. dev,)</span>
0043 <span class="comment">%                * 'auc': negative area under ROC, for 2-class outcomes</span>
0044 <span class="comment">%                * 'cond_entropy': conditional entropy of Target given Prediction, for discrete outcomes</span>
0045 <span class="comment">%                * 'cross_entropy': cross-entropy of Target under Prediction</span>
0046 <span class="comment">%                * 'f_measure': negative F-score, harmonic mean between precision and recall, for 2-class outcomes</span>
0047 <span class="comment">%                * cell array of strings: compute loss for each specified type and return an array of results</span>
0048 <span class="comment">%</span>
0049 <span class="comment">%   Target     : target variable; can be in any of the formats produced by ml_predict.</span>
0050 <span class="comment">%</span>
0051 <span class="comment">%   Prediction : predicted variable; can be in any of the formats produced by ml_predict.</span>
0052 <span class="comment">%                format is expected to be consistent with (though not necessarily identical to) the Target format,</span>
0053 <span class="comment">%                and the number of samples needs to match that of the target variable</span>
0054 <span class="comment">%</span>
0055 <span class="comment">% Out:</span>
0056 <span class="comment">%   Loss       : the scalar loss, aggregated over all samples, or a vector of loss measures, if a cell array was specified for the type</span>
0057 <span class="comment">%   Stats      : struct of additional statistics (depending on type), or a struct array if a cell array was specified for the type</span>
0058 <span class="comment">%</span>
0059 <span class="comment">% Note:</span>
0060 <span class="comment">%   Simple of the implemented losses have not been tested for all applicable combinations of target/prediction formats. For standard applications,</span>
0061 <span class="comment">%   there should be no errors, however. When new losses are implemented, they should be added to this file.</span>
0062 <span class="comment">%</span>
0063 <span class="comment">% References:</span>
0064 <span class="comment">%   [1] MacKay, D. J. C. &quot;Information theory, inference, and learning algorithms.&quot;</span>
0065 <span class="comment">%       Cambridge University Press, 2003.</span>
0066 <span class="comment">%</span>
0067 <span class="comment">%                               Christian Kothe, Swartz Center for Computational Neuroscience, UCSD</span>
0068 <span class="comment">%                               2010-04-22</span>
0069 
0070 
0071 
0072 <span class="comment">% the default metric; if type is empty,</span>
0073 max_nll_loss = 1000; <span class="comment">% to prevent negative log-likelihoods from blowing up</span>
0074 
0075 <span class="keyword">if</span> iscell(type) &amp;&amp; all(cellfun(@ischar,type))
0076     <span class="comment">% call recursively for each type and aggregate statistics</span>
0077     <span class="keyword">for</span> t=1:length(type)
0078         [measure(t),stats{t}] = <a href="ml_calcloss.html" class="code" title="function [measure,stats] = ml_calcloss(type,T,P)">ml_calcloss</a>(type{t},T,P); <span class="keyword">end</span>
0079     measure = measure(1);
0080     stats = hlp_aggregatestructs(stats,<span class="string">'replace'</span>);
0081 <span class="keyword">else</span>
0082     <span class="comment">% add some stats</span>
0083     <span class="keyword">if</span> is_discrete(T)
0084         stats.classes = T{3};
0085         stats.class_ratio = mean(T{2});
0086     <span class="keyword">elseif</span> is_discrete(P) &amp;&amp; is_point(T)
0087         stats.classes = P{3};
0088         <span class="keyword">for</span> c=1:length(stats.classes)
0089             stats.class_ratio(c) = mean(T==stats.classes(c)); <span class="keyword">end</span>
0090     <span class="keyword">end</span>
0091     
0092     <span class="comment">% auto-determine measure type</span>
0093     <span class="keyword">if</span> isempty(type) || strcmp(type,<span class="string">'auto'</span>)
0094         <span class="keyword">if</span> is_1d_regression(T) &amp;&amp; is_1d_regression(P)
0095             <span class="comment">% we have regression outputs in both cases -&gt; mean squared error</span>
0096             type = <span class="string">'mse'</span>;
0097         <span class="keyword">elseif</span> is_1d_regression(T) &amp;&amp; is_discrete(P)
0098             <span class="comment">% the targets are regression values, the estimates are discrete posteriors -&gt; misclassification rate</span>
0099             type = <span class="string">'mcr'</span>;
0100         <span class="keyword">elseif</span> (is_1d_regression(T) &amp;&amp; is_1d_distributed(P)) <span class="keyword">...</span>
0101                 || (is_1d_distributed(T) &amp;&amp; is_1d_regression(P))
0102             <span class="comment">% one is a distribution, the other is a point estimate -&gt; negative log-likelihood</span>
0103             type = <span class="string">'nll'</span>;
0104         <span class="keyword">elseif</span> is_1d_distributed(T) &amp;&amp; is_1d_distributed(P)
0105             <span class="comment">% both are 1d distributions (hopefully of compatible types -&gt; kullback-leibler divergence)</span>
0106             type = <span class="string">'kld'</span>;
0107         <span class="keyword">elseif</span> is_discrete(T) &amp;&amp; is_discrete(P)
0108             <span class="comment">% both are discrete distributions -&gt; kullback-leibler divergence</span>
0109             type = <span class="string">'kld'</span>;
0110         <span class="keyword">elseif</span> is_structured(T) &amp;&amp; is_structured(P)
0111             <span class="comment">% both are structured distributions -&gt; misclassification rate</span>
0112             type = <span class="string">'mcr'</span>;
0113         <span class="keyword">else</span>
0114             error(<span class="string">'an appropriate measure cannot be auto-determined from the data; please specify.'</span>);
0115         <span class="keyword">end</span>
0116     <span class="keyword">end</span>
0117     
0118     <span class="keyword">switch</span> lower(type)
0119         <span class="comment">% compute distribution measures</span>
0120         <span class="keyword">case</span> <span class="string">'kld'</span>
0121             <span class="comment">% kullback-leibler divergence D_KL(targ||pred)</span>
0122             <span class="keyword">if</span> is_discrete(T) &amp;&amp; is_discrete(P)
0123                 measure = mean(<a href="#_sub4" class="code" title="subfunction res = kl_divergence_discrete(P,Q)">kl_divergence_discrete</a>(T,P));
0124             <span class="keyword">elseif</span> is_discrete(P) &amp;&amp; is_point(T) &amp;&amp; length(unique(T)) &lt;= length(P{3})
0125                 <span class="comment">% convert T into a discrete probability distribution</span>
0126                 tmp = zeros(max(T),length(T)); tmp(T+max(T).*(0:(length(T)-1))') = 1; 
0127                 T = {<span class="string">'disc'</span> tmp' P{3}};
0128                 measure = mean(<a href="#_sub4" class="code" title="subfunction res = kl_divergence_discrete(P,Q)">kl_divergence_discrete</a>(T,P));
0129             <span class="keyword">elseif</span> is_gaussian(T) &amp;&amp; is_gaussian(P)
0130                 measure = mean(<a href="#_sub5" class="code" title="subfunction res = kl_divergence_gaussian(P,Q)">kl_divergence_gaussian</a>(T,P));
0131             <span class="keyword">elseif</span> is_1d_distributed(T) &amp;&amp; is_1d_distributed(P)
0132                 <span class="comment">% use the cdf method</span>
0133                 error(<span class="string">'not yet implemented'</span>);
0134             <span class="keyword">else</span>
0135                 error(<span class="string">'cannot compute the kullback-leibler divergence for the given training &amp; prediction data format.'</span>);
0136             <span class="keyword">end</span>
0137         <span class="keyword">case</span> <span class="string">'nll'</span>
0138             <span class="comment">% negative log-likelihood</span>
0139             <span class="keyword">if</span> is_point(T) &amp;&amp; is_distributed(P)
0140                 measure = sum(<a href="#_sub3" class="code" title="subfunction V = neg_loglike(X,D,maxval)">neg_loglike</a>(T,P,max_nll_loss));
0141             <span class="keyword">elseif</span> is_distributed(T) &amp;&amp; is_point(P)
0142                 measure = sum(<a href="#_sub3" class="code" title="subfunction V = neg_loglike(X,D,maxval)">neg_loglike</a>(P,T,max_nll_loss));
0143             <span class="keyword">else</span>
0144                 error(<span class="string">'cannot compute log-likelihood for the given training and prediction data format.'</span>);
0145             <span class="keyword">end</span>
0146         <span class="keyword">case</span> {<span class="string">'mcr'</span>,<span class="string">'err'</span>}
0147             <span class="comment">% misclassification rate</span>
0148             <span class="keyword">if</span> is_structured(T) &amp;&amp; is_structured(P)
0149                 measure = mean(cellfun(@isequal,T,P));
0150             <span class="keyword">else</span>
0151                 <span class="comment">% we can enumerate classes &amp; compute a per-class error</span>
0152                 <span class="keyword">if</span> isnumeric(T) &amp;&amp; isnumeric(P) &amp;&amp; isequal(size(T),size(P)) &amp;&amp; size(T,2) == 1
0153                     classes = unique([T;P]);
0154                 <span class="keyword">elseif</span> (is_1d_regression(T) || is_discrete(T)) &amp;&amp; (is_1d_regression(P) || is_discrete(P))
0155                     <span class="keyword">if</span> is_discrete(T)
0156                         classes = T{3};
0157                         T = T{3}(hlp_getresult(2,@max,T{2}')');
0158                     <span class="keyword">end</span>
0159                     <span class="keyword">if</span> is_discrete(P)
0160                         classes = P{3};
0161                         P = P{3}(hlp_getresult(2,@max,P{2}')');
0162                     <span class="keyword">end</span>
0163                 <span class="keyword">else</span>
0164                     error(<span class="string">'target and/or prediction formats not suited for classification'</span>);
0165                 <span class="keyword">end</span>
0166                 <span class="keyword">try</span>
0167                     corrects = T == P;
0168                 <span class="keyword">catch</span>
0169                     disp(<span class="string">'Data unaligned; working around this; this is a severe bug.'</span>);
0170                     len = max(size(T,1),size(P,1));
0171                     T = T(round((1:len)*size(T,1)/len));
0172                     P = P(round((1:len)*size(P,1)/len));
0173                     corrects = T == P;
0174                 <span class="keyword">end</span>
0175                     
0176                 measure = 1-mean(corrects);
0177                 <span class="comment">% also add per-class error</span>
0178                 stats.classes = classes;
0179                 stats.per_class = nan(size(classes,1),1);
0180                 <span class="keyword">for</span> c=1:length(classes)
0181                     <span class="keyword">if</span> any(T==classes(c))
0182                         stats.per_class(c) = 1-mean(corrects(T==c)); <span class="keyword">end</span>
0183                 <span class="keyword">end</span>
0184                 <span class="keyword">if</span> length(classes) == 2
0185                     stats.TP = sum(T==classes(2) &amp; P==classes(2))/sum(T==classes(2));
0186                     stats.TN = sum(T==classes(1) &amp; P==classes(1))/sum(T==classes(1));
0187                     stats.FP = sum(T==classes(1) &amp; P==classes(2))/sum(T==classes(1)); <span class="comment">% fixed FP/FN rates, reported by Matt Peterson.</span>
0188                     stats.FN = sum(T==classes(2) &amp; P==classes(1))/sum(T==classes(2));
0189                 <span class="keyword">end</span>
0190             <span class="keyword">end</span>
0191             
0192         <span class="keyword">otherwise</span>
0193             <span class="comment">% compute sample-based measures</span>
0194             Tx = <a href="#_sub2" class="code" title="subfunction V = expected_value(D)">expected_value</a>(T);
0195             Px = <a href="#_sub2" class="code" title="subfunction V = expected_value(D)">expected_value</a>(P);
0196             <span class="keyword">switch</span> lower(type)
0197                 <span class="keyword">case</span> {<span class="string">'mae'</span>,<span class="string">'l1'</span>}
0198                     measure = mean(abs(Px-Tx));
0199                 <span class="keyword">case</span> {<span class="string">'smae'</span>}
0200                     measure = mean(abs(Px-Tx)) ./ std(Tx);
0201                 <span class="keyword">case</span> {<span class="string">'mse'</span>,<span class="string">'l2'</span>}
0202                     measure = mean((Px-Tx).^2);
0203                 <span class="keyword">case</span> <span class="string">'sign'</span>
0204                     measure = mean(sign(Px) ~= sign(Tx));
0205                 <span class="keyword">case</span> {<span class="string">'smse'</span>}
0206                     measure = mean((Px-Tx).^2) ./ var(Tx);
0207                 <span class="keyword">case</span> {<span class="string">'max'</span>,<span class="string">'linf'</span>}
0208                     measure = max(abs(Px-Tx));
0209                 <span class="keyword">case</span> <span class="string">'rms'</span>
0210                     measure = sqrt(mean((Px-Tx).^2));
0211                 <span class="keyword">case</span> <span class="string">'bias'</span>
0212                     measure = mean(Px-Tx);
0213                 <span class="keyword">case</span> <span class="string">'medse'</span>
0214                     measure = median((Px-Tx).^2);
0215                 <span class="keyword">case</span> <span class="string">'medae'</span>
0216                     measure = median(abs(Px-Tx));
0217                 <span class="keyword">case</span> <span class="string">'smedae'</span>
0218                     measure = median(abs(Px-Tx)) ./ (<a href="#_sub1" class="code" title="subfunction Y = mad(X,flag) ">mad</a>(Tx,1)*1.4826);
0219                 <span class="keyword">case</span> <span class="string">'auc'</span>
0220                     <span class="comment">% derive binary T, continuous P, and the class identities</span>
0221                     <span class="keyword">if</span> is_discrete(P)
0222                         classes = P{3};
0223                         P = P{2}(:,2);
0224                         <span class="keyword">if</span> is_discrete(T)
0225                             T = T{2}(:,2);
0226                         <span class="keyword">elseif</span> ~is_1d_regression(T)
0227                             error(<span class="string">'AUC cannot be computed for non-class targets'</span>);
0228                         <span class="keyword">end</span>
0229                     <span class="keyword">elseif</span> is_discrete(T) &amp;&amp; is_1d_regression(P)
0230                         classes = T{3};
0231                         T = T{2}(:,2);
0232                     <span class="keyword">elseif</span> is_1d_regression(T) &amp;&amp; is_1d_regression(P)
0233                         classes = unique(T);
0234                     <span class="keyword">end</span>
0235                     <span class="comment">% compute the AUC</span>
0236                     <span class="keyword">if</span> length(classes) ~= 2
0237                         warning(<span class="string">'AUC can only be computed for 2-class outcomes. Returning NaN.'</span>); 
0238                         measure = NaN;
0239                     <span class="keyword">else</span>
0240                         <span class="keyword">if</span> length(unique(T)) &gt; 2
0241                             error(<span class="string">'AUC can only be computed for nonprobabilistic targets.'</span>); <span class="keyword">end</span>
0242                         T = T==classes(2);
0243                         nT = sum(T);
0244                         ranks = tied_ranks(P);
0245                         measure = -(sum(ranks(T)) - (nT^2 + nT)/2) / (nT*(length(T)-nT));
0246                     <span class="keyword">end</span>
0247                 <span class="keyword">case</span> <span class="string">'f_measure'</span>
0248                     classes = [];
0249                     <span class="keyword">if</span> is_discrete(T)
0250                         classes = T{3};
0251                         T = T{2}*T{3};
0252                     <span class="keyword">elseif</span> is_discrete(P) 
0253                         classes = P{3};
0254                         P = P{2}*P{3};
0255                     <span class="keyword">elseif</span> is_point(T)
0256                         classes = unique(T);
0257                     <span class="keyword">end</span>
0258                     <span class="keyword">if</span> length(classes) ~= 2
0259                         error(<span class="string">'computation of the f measure requires 2-class outcomes'</span>); <span class="keyword">end</span>
0260                     T = T==classes(2);
0261                     P = P==classes(2);
0262                     stats.TP = sum((P==1)&amp;(T==1));
0263                     stats.TN = sum((P==0)&amp;(T==0));
0264                     stats.FP = sum((P==1)&amp;(T==0));
0265                     stats.FN = sum((P==0)&amp;(T==1));
0266                     measure = -2*stats.TP/(2*stats.TP+stats.FP+stats.FN);
0267                 <span class="keyword">case</span> <span class="string">'cross_entropy'</span>
0268                     <span class="keyword">if</span> is_discrete(P) &amp;&amp; is_1d_regression(T)
0269                         <span class="comment">% convert T into a discrete distribution</span>
0270                         T = {<span class="string">'disc'</span> repmat(T,1,length(P{3})) == repmat(P{3}',size(T,1),1) P{3}};
0271                     <span class="keyword">elseif</span> is_discrete(T) &amp;&amp; is_1d_regression(P)
0272                         <span class="comment">% convert P into a discrete distribution</span>
0273                         <span class="keyword">if</span> length(unique(P)) &gt; length(T{3})
0274                             error(<span class="string">'P must contain class predictions, not gradual predictions'</span>); <span class="keyword">end</span>
0275                         P = {<span class="string">'disc'</span> repmat(P,1,length(T{3})) == repmat(T{3}',size(P,1),1) T{3}};
0276                     <span class="keyword">elseif</span> ~(is_discrete(T) || is_discrete(P))
0277                         error(<span class="string">'cross-entropy can currently only be computed for class outcomes.'</span>);
0278                     <span class="keyword">end</span>
0279                     measure = mean(<a href="#_sub4" class="code" title="subfunction res = kl_divergence_discrete(P,Q)">kl_divergence_discrete</a>(T,P));
0280                 <span class="keyword">case</span> <span class="string">'cond_entropy'</span>
0281                     error(<span class="string">'not yet implemented.'</span>);
0282                 <span class="keyword">otherwise</span>
0283                     error(<span class="string">'unknown measure specified.'</span>);
0284             <span class="keyword">end</span>
0285     <span class="keyword">end</span>
0286 <span class="keyword">end</span>
0287 stats.measure = type;
0288 stats.(type) = measure;
0289 <span class="keyword">end</span>
0290 
0291 
0292 <a name="_sub1" href="#_subfunctions" class="code">function Y = mad(X,flag) </a><span class="comment">%#ok&lt;INUSD&gt;</span>
0293 Y = median(abs(bsxfun(@minus,X,median(X))));
0294 <span class="keyword">end</span>
0295 
0296 <span class="comment">% expectation of distribution D</span>
0297 <a name="_sub2" href="#_subfunctions" class="code">function V = expected_value(D)</a>
0298 <span class="keyword">if</span> ~is_distributed(D)
0299     V = D;
0300 <span class="keyword">elseif</span> is_discrete(D)
0301     V = D{2}*D{3}(:);
0302 <span class="keyword">elseif</span> is_1d_distributed(D)
0303     params = mat2cell(D{2}, size(D{2},1), ones(1,size(D{2},2)));
0304     V = feval([D{1},<span class="string">'stat'</span>], params{:});
0305 <span class="keyword">elseif</span> is_Nd_distributed(D)
0306     means = cellfun(@(x)x{1}',D{2},<span class="string">'UniformOutput'</span>,false);
0307     V = vertcat(means{:});
0308 <span class="keyword">else</span>
0309     error(<span class="string">'unsupported distribution format; cannot compute expected value'</span>);
0310 <span class="keyword">end</span>
0311 <span class="keyword">end</span>
0312 
0313 
0314 
0315 <span class="comment">% negative log-likelihood of X under D</span>
0316 <a name="_sub3" href="#_subfunctions" class="code">function V = neg_loglike(X,D,maxval)</a>
0317 <span class="keyword">if</span> ~exist(<span class="string">'maxval'</span>,<span class="string">'var'</span>)
0318     maxval = Inf; <span class="keyword">end</span>
0319 <span class="keyword">if</span> is_discrete(D)
0320     match = repmat(X,1,size(D{3},1)) == repmat(D{3}',size(X,1),1);
0321     V = -log(D{2}(match));
0322 <span class="keyword">elseif</span> is_1d_distributed(D)
0323     params = mat2cell(D{2}, size(D{2},1), ones(1,size(D{2},2)));
0324     V = -log(feval([D{1},<span class="string">'pdf'</span>],X,params{:}));
0325 <span class="keyword">elseif</span> is_Nd_distributed(D)
0326     V = zeros(size(X,1),1);
0327     <span class="keyword">for</span> k=1:size(X,1)
0328         V(k) = mvnpdf(X(k,:),D{2}{k}{1},D{2}{k}{2}); <span class="keyword">end</span>
0329 <span class="keyword">else</span>
0330     error(<span class="string">'the second argument must be a probability distribution'</span>);
0331 <span class="keyword">end</span>
0332 V(isinf(V)) = maxval;
0333 <span class="keyword">end</span>
0334 
0335 
0336 
0337 
0338 <span class="comment">% compute the KL divergence D_KL(P,Q) for sets of discrete probability distributions P, Q</span>
0339 <a name="_sub4" href="#_subfunctions" class="code">function res = kl_divergence_discrete(P,Q)</a>
0340 <span class="keyword">if</span> ~is_discrete(P) || ~is_discrete(Q)
0341     error(<span class="string">'computation of the discrete KL divergence requires discrete distributions.'</span>); <span class="keyword">end</span>
0342 <span class="comment">% extract probability mass functions (and renormalize for good measure)</span>
0343 P = P{2}./repmat(sum(P{2},2),1,size(P{2},2));
0344 Q = Q{2}./repmat(sum(Q{2},2),1,size(Q{2},2));
0345 <span class="keyword">if</span> ~isequal(size(P),size(Q))
0346     error(<span class="string">'P and Q must have the same number of classes/bins and samples'</span>); <span class="keyword">end</span>
0347 <span class="keyword">if</span> any(~isfinite(P(:))) || any(~isfinite(Q(:)))
0348     error(<span class="string">'the inputs contain non-finite values!'</span>); <span class="keyword">end</span>
0349 res = P.*log(P./Q);
0350 res(isnan(res)) = 0;
0351 res = sum(res,2);
0352 <span class="keyword">end</span>
0353 
0354 
0355 
0356 <span class="comment">% compute the KL divergence D_KL(P,Q) for sets of gaussian probability distributions P,Q</span>
0357 <a name="_sub5" href="#_subfunctions" class="code">function res = kl_divergence_gaussian(P,Q)</a>
0358 <span class="keyword">if</span> ~is_gaussian(P) || ~is_gaussian(Q)
0359     error(<span class="string">'P and Q must contain gaussian distributions'</span>); <span class="keyword">end</span>
0360 <span class="comment">% re-represent univariate distributions as a multivariate ones</span>
0361 P = quickif(strcmp(P{1},<span class="string">'norm'</span>), cellfun(@(x){x(1),x(2)},mat2cell(P{2},ones(size(P{2},1),1),2),<span class="string">'UniformOutput'</span>,false), P{2});
0362 Q = quickif(strcmp(Q{1},<span class="string">'norm'</span>), cellfun(@(x){x(1),x(2)},mat2cell(Q{2},ones(size(Q{2},1),1),2),<span class="string">'UniformOutput'</span>,false), Q{2});
0363 <span class="keyword">if</span> length(P) ~= length(Q)
0364     error(<span class="string">'P and Q must have the same number of distributions.'</span>); <span class="keyword">end</span>
0365 res = zeros(length(P),1);
0366 <span class="keyword">for</span> i=1:length(P)
0367     [mu0,sig0] = P{i}{:};
0368     [mu1,sig1] = Q{i}{:};
0369     res(i) = (logdet(sig1)-logdet(sig0) + trace(sig1\sig0) + (mu1-mu0)' * (sig1\(mu1-mu0)) - length(mu0)) / 2;
0370 <span class="keyword">end</span>
0371 <span class="keyword">end</span></pre></div>

<hr><address>Generated on Wed 19-Aug-2015 18:06:23 by <strong><a href="http://www.artefact.tk/software/matlab/m2html/" title="Matlab Documentation in HTML">m2html</a></strong> &copy; 2005</address>
</body>
</html>